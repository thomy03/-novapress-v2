# NovaPress AI v2 - Scraping Avanc√© & Respect des R√®gles

## üéØ Objectif

Cr√©er des **synth√®ses originales** en scrappant directement les articles de journaux mondiaux, tout en **respectant strictement** les r√®gles l√©gales et √©thiques.

## ‚úÖ Respect des R√®gles

### 1. robots.txt

```python
# V√©rification automatique avant CHAQUE requ√™te
if not self._check_robots_txt(domain, url):
    logger.warning(f"Scraping not allowed by robots.txt: {url}")
    return None
```

**Comment √ßa marche:**
- T√©l√©charge et parse le `robots.txt` de chaque site
- Cache les r√®gles pour √©viter de recharger
- Respecte les interdictions (`Disallow`)
- Utilise User-Agent: `NovaPress/2.0`

### 2. Rate Limiting Intelligent

```python
# D√©lai entre requ√™tes par domaine
RATE_LIMITS = {
    "lemonde.fr": 1.0,      # 1 sec entre requ√™tes
    "nytimes.com": 2.0,     # 2 sec (plus conservateur)
    "theguardian.com": 1.0
}
```

**Pourquoi:**
- √âvite de surcharger les serveurs
- Respecte les ressources des sites
- √âvite les bannissements IP

### 3. D√©tection de Paywall

```python
# Si paywall d√©tect√©, on SKIP l'article
if self._detect_paywall(html, url):
    logger.warning(f"Paywall detected: {url}")
    return None  # On ne contourne PAS
```

**Contenu paywall:**
- ‚ùå ON NE CONTOURNE PAS
- ‚ùå ON N'UTILISE PAS de techniques de bypass
- ‚úÖ On scrape uniquement le contenu librement accessible

### 4. User-Agent Transparent

```python
headers = {
    "User-Agent": "NovaPress/2.0 (+https://novapress.ai)"
}
```

**Identification claire:**
- Nom du bot
- URL du projet
- Pas de masquage en navigateur

### 5. D√©duplication √âthique

```python
# On garde le MEILLEUR article, pas tous les doublons
if await self._is_duplicate(title, content):
    return None  # Skip duplicate
```

**Pourquoi:**
- √âvite de surcharger le stockage
- Respecte les sources originales
- Synth√®ses de meilleure qualit√©

---

## üåç Sources Scrapp√©es (10 journaux mondiaux)

### Fran√ßais
- **Le Monde** (lemonde.fr)
- **Le Figaro** (lefigaro.fr)
- **Lib√©ration** (liberation.fr)

### Anglais
- **The New York Times** (nytimes.com)
- **The Guardian** (theguardian.com)
- **BBC News** (bbc.com)
- **Reuters** (reuters.com)

### Autres Langues
- **Der Spiegel** (spiegel.de) - Allemand
- **El Pa√≠s** (elpais.com) - Espagnol
- **Corriere della Sera** (corriere.it) - Italien

---

## üîß Fonctionnement du Pipeline V4

### √âtape 1: D√©couverte d'Articles

```python
# Visite la page d'accueil de chaque source
articles_urls = await scraper.discover_article_urls("lemonde.fr", max_articles=20)

# R√©sultat: Liste d'URLs d'articles r√©cents
# ['https://lemonde.fr/article/...', ...]
```

**Processus:**
1. Charge la homepage
2. Parse HTML avec BeautifulSoup
3. Extrait liens d'articles (pas sections/cat√©gories)
4. Filtre URLs valides
5. Retourne max 20 URLs

### √âtape 2: Scraping d'Articles

```python
# Pour chaque URL d√©couverte
article_data = await scraper.scrape_article(url)

# Extraction avec Newspaper3k
{
    "url": "https://...",
    "source_name": "Le Monde",
    "raw_title": "Titre de l'article",
    "raw_text": "Contenu complet...",
    "summary": "R√©sum√© auto-extrait",
    "published_at": "2025-01-19T...",
    "authors": ["Auteur 1", "Auteur 2"],
    "image_url": "https://...",
    "language": "fr"
}
```

**Newspaper3k** fait:
- T√©l√©chargement HTML
- D√©tection automatique du contenu principal
- Extraction auteurs, date, images
- Nettoyage HTML (retire pub, menus, etc.)

### √âtape 3: D√©duplication Intelligente

```python
# D√©tecte doublons par embeddings
similarity_matrix = compute_similarity(embeddings)

# Groupes d'articles similaires
groups = [[0, 5, 12],  # M√™me √©v√©nement, 3 sources diff√©rentes
          [3, 8],      # M√™me sujet, 2 sources
          [15, 20]]    # Doublons

# Garde le meilleur de chaque groupe
for group in groups:
    best = select_best_article(articles, group)
    keep.append(best)
```

**Crit√®res de s√©lection:**
1. **Longueur** (40%) - Article le plus complet
2. **Image** (20%) - Pr√©sence d'illustration
3. **Source** (30%) - Source r√©put√©e (NYT, Guardian, Le Monde...)
4. **Fra√Æcheur** (10%) - Plus r√©cent

**R√©sultat:**
- 100 articles scrap√©s ‚Üí 60 articles uniques
- Taux de d√©duplication: ~40%

### √âtape 4: Embeddings BGE-M3

```python
# Vectorisation pour similarit√© s√©mantique
embeddings = bge_m3.encode(articles)
# Shape: (60, 1024)
```

**BGE-M3:**
- Multilingue (fran√ßais, anglais, etc.)
- 1024 dimensions
- √âtat de l'art pour retrieval

### √âtape 5: Clustering HDBSCAN

```python
# Regroupe articles par th√®me
clusters = hdbscan.fit_predict(embeddings)

# R√©sultat: 8 clusters th√©matiques
# Cluster 0: IA et technologie (15 articles)
# Cluster 1: √âconomie mondiale (12 articles)
# Cluster 2: G√©opolitique (8 articles)
# ...
```

**HDBSCAN:**
- D√©tection automatique du nombre de clusters
- Pas besoin de sp√©cifier K
- Robuste au bruit

### √âtape 6: Knowledge Graph

```python
# Extraction d'entit√©s avec spaCy
entities = spacy_ner.extract(articles)

# Construction du graphe
graph = {
    "nodes": [
        {"id": "node_0", "label": "Emmanuel Macron", "type": "PERSON"},
        {"id": "node_1", "label": "Union Europ√©enne", "type": "ORG"},
        ...
    ],
    "edges": [
        {"source": "node_0", "target": "node_1", "label": "CO_OCCURS"},
        ...
    ]
}
```

**spaCy fr_core_news_lg:**
- Reconnaissance d'entit√©s nomm√©es
- Personnes, organisations, lieux, √©v√©nements
- Pr√©cision: 92% sur fran√ßais

### √âtape 7: Synth√®se AI (Ollama/Mistral)

```python
# Pour chaque cluster (top 10)
synthesis = await mistral.generate_synthesis(cluster_articles)

# R√©sultat
{
    "title": "Intelligence Artificielle : les r√©gulations se multiplient",
    "summary": "Face √† l'essor de l'IA, les gouvernements mondiaux...",
    "keyPoints": [
        "L'UE finalise son AI Act",
        "Les √âtats-Unis annoncent de nouvelles r√®gles",
        "D√©bat sur l'IA g√©n√©rative et droits d'auteur"
    ],
    "sources": ["Le Monde", "NYT", "Guardian"],
    "complianceScore": 95,
    "readingTime": 3
}
```

**Ollama/Mistral:**
- LLM local (pas d'API externe)
- G√©n√©ration de synth√®ses factuelles
- Respect des sources
- Aucun plagiat

---

## üìä Exemple Complet

```python
from app.services.pipeline import get_pipeline_engine

# Initialiser
pipeline = get_pipeline_engine()
await pipeline.initialize()

# Ex√©cuter pipeline
results = await pipeline.run_full_pipeline(
    mode="SCRAPE",  # Scraper les sources
    sources=["lemonde.fr", "nytimes.com", "theguardian.com"],
    max_articles_per_source=20
)

# R√©sultats
print(f"Articles scrap√©s: {results['stats']['total_scraped']}")
print(f"Articles uniques: {results['stats']['unique_articles']}")
print(f"Clusters: {results['stats']['clusters_found']}")
print(f"Synth√®ses: {results['stats']['syntheses_generated']}")
print(f"Sources: {results['stats']['sources_used']}")
```

**Output attendu:**
```
üöÄ NovaPress Pipeline V4 ULTIMATE (Mode: SCRAPE)
üì° Step 1: Advanced Web Scraping...
   Discovered 20 articles from Le Monde
   Discovered 20 articles from The New York Times
   Discovered 18 articles from The Guardian
   ‚úÖ Scraped 48 articles from 3 sources

üßÆ Step 2: Computing embeddings...
   ‚úÖ Generated 48 embeddings (1024-dim)

üîç Step 3: Intelligent deduplication...
   ‚úÖ 32 unique articles (16 duplicates removed)

üîó Step 4: Clustering...
   ‚úÖ Found 6 thematic clusters

üï∏Ô∏è Step 5: Knowledge Graph...
   ‚úÖ Graph: 45 entities, 87 relations

‚úçÔ∏è Step 6: AI Syntheses...
   Generating synthesis for cluster 0 (8 articles)
   Generating synthesis for cluster 1 (7 articles)
   ...
   ‚úÖ Generated 6 AI syntheses

üíæ Step 7: Storage...
   ‚úÖ Articles stored in vector database

üéâ Pipeline completed in 42.3s!
```

---

## üîí Garanties L√©gales & √âthiques

### ‚úÖ CE QU'ON FAIT

1. **Scraping Transparent**
   - User-Agent identifiable
   - Respect robots.txt
   - Rate limiting

2. **Contenu Libre**
   - Uniquement contenu public
   - Pas de contournement paywall
   - Attribution des sources

3. **Transformation Creative**
   - Synth√®ses originales (pas de copie)
   - Analyse s√©mantique propre
   - Clustering intelligent
   - Graphes de connaissances

4. **Fair Use**
   - Usage informatif/√©ducatif
   - Transformation substantielle
   - Pas de reproduction int√©grale

### ‚ùå CE QU'ON NE FAIT PAS

1. **Pas de contournement**
   - Pas de bypass paywall
   - Pas de masquage User-Agent
   - Pas d'abus de fr√©quence

2. **Pas de plagiat**
   - Pas de copie d'articles entiers
   - Pas de republication sans transformation
   - Attribution syst√©matique

3. **Pas de surcharge**
   - Rate limiting strict
   - Concurrence limit√©e
   - Cache pour √©viter re-scraping

---

## üéì Aspect L√©gal (Fair Use / Exception de Citation)

### En France: Exception de Citation

**Article L122-5 du CPI:**
> "Les analyses et courtes citations justifi√©es par le caract√®re critique, pol√©mique, p√©dagogique, scientifique ou d'information de l'≈ìuvre √† laquelle elles sont incorpor√©es"

**NovaPress respecte:**
- ‚úÖ Citation courte (r√©sum√©s, pas articles entiers)
- ‚úÖ Caract√®re informatif
- ‚úÖ Attribution de la source
- ‚úÖ Transformation substantielle (synth√®se AI)

### Aux USA: Fair Use

**17 U.S. Code ¬ß 107:**
1. **Purpose**: Informatif, √©ducatif ‚úÖ
2. **Nature**: ≈íuvres factuelles (news) ‚úÖ
3. **Amount**: Portions raisonnables (r√©sum√©s) ‚úÖ
4. **Effect**: Pas de concurrence directe ‚úÖ

---

## üìà Performance

| M√©trique | Valeur |
|----------|--------|
| Sources scrap√©es | 10 journaux mondiaux |
| Articles/source | ~20 |
| Total brut | ~200 articles |
| Apr√®s d√©duplication | ~120 articles uniques |
| Clusters d√©tect√©s | ~15 th√©matiques |
| Synth√®ses g√©n√©r√©es | ~10-15 |
| Temps pipeline | ~45-60 secondes |

---

## üöÄ Utilisation

```bash
# Mode SCRAPE (journaux mondiaux)
python -m app.services.pipeline --mode=SCRAPE

# Mode TOPIC (recherche th√©matique)
python -m app.services.pipeline --mode=TOPIC --topics="IA,√âconomie"

# Mode SIMULATION (test)
python -m app.services.pipeline --mode=SIMULATION
```

---

**NovaPress AI v2 - Scraping Intelligent & √âthique** üåç‚ú®
